{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_Grad_CAM_Hamza_Gharbi.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dn4qdHvF3NjH"
      },
      "source": [
        "## Visualization of CNN: Grad-CAM\n",
        "* **Objective**: Convolutional Neural Networks are widely used on computer vision. It is powerful for processing grid-like data. However we hardly know how and why it works, due to the lack of decomposability into individually intuitive components. In this assignment, we will introduce the Grad-CAM which visualizes the heatmap of input images by highlighting the important region for visual question answering(VQA) task.\n",
        "\n",
        "* **To be submitted**: this notebook in two weeks, **cleaned** (i.e. without results, for file size reasons: `menu > kernel > restart and clean`), in a state ready to be executed (if one just presses 'Enter' till the end, one should obtain all the results for all images) with a few comments at the end. No additional report, just the notebook!\n",
        "\n",
        "* NB: if `PIL` is not installed, try `conda install pillow`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HYwDbvGr3NjO"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "warnings.filterwarnings('ignore')\n",
        "import cv2\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5TPEOxy3NjQ"
      },
      "source": [
        "### Visual Question Answering problem\n",
        "Given an image and a question in natural language, the model choose the most likely answer from 3 000 classes according to the content of image. The VQA task is indeed a multi-classificaition problem.\n",
        "<img src=\"vqa_model.PNG\">\n",
        "\n",
        "We provide you a pretrained model `vqa_resnet` for VQA tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "znF7xyZY3NjQ",
        "outputId": "34c5ba4a-e1f6-4762-bb00-f4d1ea250ee6"
      },
      "source": [
        "# load model\n",
        "from load_model import load_model\n",
        "vqa_resnet = load_model()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-163b84fc54d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mload_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvqa_resnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'load_model'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgoW1QxI3NjR"
      },
      "source": [
        "print(vqa_resnet) # for more information "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IttAELee3NjR"
      },
      "source": [
        "checkpoint = '2017-08-04_00.55.19.pth'\n",
        "saved_state = torch.load(checkpoint, map_location=device)\n",
        "# reading vocabulary from saved model\n",
        "vocab = saved_state['vocab']\n",
        "\n",
        "# reading word tokens from saved model\n",
        "token_to_index = vocab['question']\n",
        "\n",
        "# reading answers from saved model\n",
        "answer_to_index = vocab['answer']\n",
        "\n",
        "num_tokens = len(token_to_index) + 1\n",
        "\n",
        "# reading answer classes from the vocabulary\n",
        "answer_words = ['unk'] * len(answer_to_index)\n",
        "for w, idx in answer_to_index.items():\n",
        "    answer_words[idx]=w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q0bQf973NjS"
      },
      "source": [
        "### Inputs\n",
        "In order to use the pretrained model, the input image should be normalized using `mean = [0.485, 0.456, 0.406]`, and `std = [0.229, 0.224, 0.225]`, and be resized as `(448, 448)`. You can call the function `image_to_features` to achieve image preprocessing. For input question, the function `encode_question` is provided to encode the question into a vector of indices. You can also use `preprocess` function for both image and question preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03gpEXAs3NjS"
      },
      "source": [
        "def get_transform(target_size, central_fraction=1.0):\n",
        "    return transforms.Compose([\n",
        "        transforms.Scale(int(target_size / central_fraction)),\n",
        "        transforms.CenterCrop(target_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225]),\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLrO_BY83NjS"
      },
      "source": [
        "def encode_question(question):\n",
        "    \"\"\" Turn a question into a vector of indices and a question length \"\"\"\n",
        "    question_arr = question.lower().split()\n",
        "    vec = torch.zeros(len(question_arr), device=device).long()\n",
        "    for i, token in enumerate(question_arr):\n",
        "        index = token_to_index.get(token, 0)\n",
        "        vec[i] = index\n",
        "    return vec, torch.tensor(len(question_arr), device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AG_ehsBH3NjT"
      },
      "source": [
        "# preprocess requires the dir_path of an image and the associated question. \n",
        "#It returns the spectific input form which can be used directly by vqa model. \n",
        "def preprocess(dir_path, question):\n",
        "    q, q_len = encode_question(question)\n",
        "    img = Image.open(dir_path).convert('RGB')\n",
        "    image_size = 448  # scale image to given size and center\n",
        "    central_fraction = 1.0\n",
        "    transform = get_transform(image_size, central_fraction=central_fraction)\n",
        "    img_transformed = transform(img)\n",
        "    img_features = img_transformed.unsqueeze(0).to(device)\n",
        "    \n",
        "    inputs = (img_features, q.unsqueeze(0), q_len.unsqueeze(0))\n",
        "    return inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA0yf5gv3NjT"
      },
      "source": [
        "We provide you two pictures and some question-answers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeoRISyE3NjT"
      },
      "source": [
        "Question1 = 'What animal'\n",
        "Answer1 = ['dog','cat' ]\n",
        "indices1 = [answer_to_index[ans] for ans in Answer1]# The indices of category \n",
        "img1 = Image.open('dog_cat.png')\n",
        "img1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcEDj62E3NjU"
      },
      "source": [
        "dir_path = 'dog_cat.png' \n",
        "inputs = preprocess(dir_path, Question1)\n",
        "ans = vqa_resnet(*inputs) # use model to predict the answer\n",
        "answer_idx = np.argmax(F.softmax(ans, dim=1).data.numpy())\n",
        "print(answer_words[answer_idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8TxHjEA3NjU"
      },
      "source": [
        "Question2 = 'What color'\n",
        "Answer2 = ['green','yellow' ]\n",
        "indices2 = [answer_to_index[ans] for ans in Answer2]\n",
        "img2 = Image.open('hydrant.png')\n",
        "img2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgGc0Azv3NjU"
      },
      "source": [
        "dir_path = 'hydrant.png' \n",
        "inputs = preprocess(dir_path, Question2)\n",
        "ans = vqa_resnet(*inputs) # use model to predict the answer\n",
        "answer_idx = np.argmax(F.softmax(ans, dim=1).data.numpy())\n",
        "print(answer_words[answer_idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLr1HLY-3NjV"
      },
      "source": [
        "### Grad-CAM \n",
        "* **Overview:** Given an image with a question, and a category (‘dog’) as input, we foward propagate the image through the model to obtain the `raw class scores` before softmax. The gradients are set to zero for all classes except the desired class (dog), which is set to 1. This signal is then backpropagated to the `rectified convolutional feature map` of interest, where we can compute the coarse Grad-CAM localization (blue heatmap).\n",
        "\n",
        "\n",
        "* **To Do**: Define your own function Grad_CAM to achieve the visualization of the two images. For each image, consider the answers we provided as the desired classes. Compare the heatmaps of different answers, and conclude. \n",
        "\n",
        "\n",
        "* **Hints**: \n",
        " + We need to record the output and grad_output of the feature maps to achieve Grad-CAM. In pytorch, the function `Hook` is defined for this purpose. Read the tutorial of [hook](https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks) carefully. \n",
        " + The pretrained model `vqa_resnet` doesn't have the activation function after its last layer, the output is indeed the `raw class scores`, you can use it directly. Run \"print(vqa_resnet)\" to get more information on VGG model.\n",
        " + The last CNN layer of the model is: `vqa_resnet.resnet_layer4.r_model.layer4[2].conv3` \n",
        " + The size of feature maps is 14x14, so as your heatmap. You need to project the heatmap to the original image(224x224) to have a better observation. The function `cv2.resize()` may help.  \n",
        " + Here is the link of the paper [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/pdf/1610.02391.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRC4aFzE6ogT"
      },
      "source": [
        "# Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68Is_PJTacOE"
      },
      "source": [
        "In the next cell , we will define the inputs of the Grad-CAM model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8p6xz5NE2i5"
      },
      "source": [
        "Question2 = 'What color'\n",
        "Answer2 = ['green','yellow' ]\n",
        "dir_path2 = 'hydrant.png' \n",
        "img2 = Image.open(dir_path2)\n",
        "\n",
        "Question1 = 'What animal'\n",
        "Answer1 = ['dog','cat' ]\n",
        "dir_path1 = 'dog_cat.png' \n",
        "img1 = Image.open(dir_path1)\n",
        "\n",
        "Questions = [Question1,Question2]\n",
        "Answers = [Answer1,Answer2]\n",
        "Dirs = [dir_path1,dir_path2]\n",
        "Imgs = [img1,img2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCgvGbifcRsP"
      },
      "source": [
        "Let's introduce the important functions for our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yYTTUjDcVHr"
      },
      "source": [
        "def forward_pass(self, input, output):\n",
        "    dict_['forward_activations'] = output[0]\n",
        "def backword_pass(self, grad_input, grad_output):\n",
        "    dict_['backward_activations']= grad_output[0]\n",
        "\n",
        "def grad_cam (activations,gradients) :\n",
        "    \"\"\"\n",
        "    Function that computes final 14x14 heatmap. \n",
        "    \"\"\"\n",
        "    ## calculate importance \n",
        "    importance = torch.mean(gradients.view(gradients.shape[0],-1),dim=1)\n",
        "\n",
        "    heatmap = torch.sum(importance[:,None,None] * activations,dim=0)\n",
        "    relu = torch.nn.ReLU()\n",
        "    grad_cam_output = relu(heatmap)\n",
        "    return grad_cam_output\n",
        "\n",
        "def heatmap_on_image(heatmap , img , alpha = 0.5):\n",
        "    \"\"\"\n",
        "    Create the array for the original image plus the heatmap.\n",
        "    Alpha measures the weight of the heatmap on the final image.\n",
        "    \"\"\" \n",
        "    min_ = np.min(heatmap)\n",
        "    max_ = np.max(heatmap)\n",
        "    heatmap = (heatmap - min_)/max_ ## standardize  heatmap\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
        "    final = cv2.addWeighted(heatmap, alpha, img, 1-alpha, 0)\n",
        "    return final "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIPkzuKAF047"
      },
      "source": [
        "Now for every input picture and  possible answer , we output three images : input image, raw heatmap and heatmap on image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hNpRyUTbYIg"
      },
      "source": [
        "for question,answer,dir_path,img in zip(Questions,Answers,Dirs,Imgs) :\n",
        "    ## get vqa inputs and answer indexes\n",
        "    inputs = preprocess(dir_path, question)\n",
        "    indexes = [answer_to_index[ans] for ans in answer]\n",
        "    img_array = np.array(img)\n",
        "    for k,index_answer in enumerate(indexes) :\n",
        "      \n",
        "      vqa_resnet = load_model()\n",
        "      dict_ = {} ## dict to save activations and gradients\n",
        "      \n",
        "      ## register hooks \n",
        "      vqa_resnet.resnet_layer4.r_model.layer4[2].conv3.register_forward_hook(forward_pass)\n",
        "      vqa_resnet.resnet_layer4.r_model.layer4[2].conv3.register_backward_hook(backword_pass)\n",
        "\n",
        "      output = vqa_resnet(*inputs)\n",
        "      \n",
        "      ## backpropagate through only the wanted class\n",
        "      output[:,index_answer].backward()\n",
        "\n",
        "      ## compute heatmap\n",
        "      heatmap = grad_cam(activations = dict_['forward_activations'],gradients = dict_['backward_activations'].squeeze(0))\n",
        "      ## resize\n",
        "      heatmap_resized = cv2.resize(heatmap.detach().numpy(),(img_array.shape[0],img_array.shape[1]))\n",
        "\n",
        "      ## Put gradients to zero\n",
        "      vqa_resnet.zero_grad()\n",
        "      ## create heatmap on original image \n",
        "      final_image = heatmap_on_image(heatmap_resized,img_array)\n",
        "      print('The class is %s'%(answer[k]))\n",
        "      fig,ax = plt.subplots(1,3 , figsize=(12,6))\n",
        "      ax[0].axis('off')\n",
        "\n",
        "      ax[0].imshow(img)\n",
        "      ax[0].set_title('Input image')\n",
        "      \n",
        "\n",
        "      ax[1].imshow(heatmap_resized)\n",
        "      ax[1].set_title('Raw heatmap for the class %s'%(answer[k]))\n",
        "\n",
        "      ax[2].imshow(final_image)\n",
        "      ax[2].set_title('Image plus heatmap for the class %s'%(answer[k]))\n",
        "\n",
        "      plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPvZl4gx89CS"
      },
      "source": [
        "With grad-cam, we can identify which parts of the image influenced the model when we back-propagate through only a certain class. For the first input, we can see that the model uses the correct information to identify the cat and the dog , without an overlapping between them. But we can also see that the parts relative to the corpse without head are not activated, even though these parts can present additional useful informations that the model can use to identify both classes.\n",
        "\n",
        "For the second image, we can see that the model made a good distinction and used the correct activations to identify the class green. However, if we choose to backpropagate through the class yellow, we can observe that the model \"looked\" also at the pixels corresponding to green. This may explain why the vqa model outputs the class green at the first place. It seems that it confuses the yellow  color when it is present with other colors in an image. A solution to this issue may be to crop out the yellow object and isolate it from others. \n",
        "\n",
        "Hence, grad cam can be an excellent tool to obtain informations concerning the misclassifid images. This visualization may be used to do more processing on images in order to help the model recognize better the different objects."
      ]
    }
  ]
}