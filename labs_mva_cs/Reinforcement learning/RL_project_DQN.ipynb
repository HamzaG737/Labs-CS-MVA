{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL-project-DQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFBGU97c_7iP"
      },
      "source": [
        "# Training pong Agent with deep Q learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmLuSW2lACfL"
      },
      "source": [
        "In this first notebook, you can find a part of our code for the RL project \"  Simulating atari pong game using Deepreinforcement learning methods \". This part concerns only DQN and in another notebook you can find our code for A2C model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gC5jDP0x0w6Z"
      },
      "source": [
        "import gym\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch.nn as nn \n",
        "import torch\n",
        "import pickle\n",
        "from torch.autograd import Variable\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHCHnBglyq0N"
      },
      "source": [
        "First we import environment name "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9sB1HV1yt-q"
      },
      "source": [
        "env_name = 'PongDeterministic-v4'  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NFDLMdWAhcS"
      },
      "source": [
        "Then we do image preprocessing ( part 2.1 in the project report). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSWlRk9eywql"
      },
      "source": [
        "def ProcessFrame(image):\n",
        "    \"\"\"\n",
        "    convert to grayscale , crop and resize \n",
        "    \"\"\"\n",
        "    image = image[30:-12,5:-4]\n",
        "    image = np.mean(image,axis = 2)\n",
        "    image = cv2.resize(image, (84,84),interpolation = cv2.INTER_NEAREST)\n",
        "    image = np.array(image,dtype = np.uint8)\n",
        "    return image[:,:,None]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2UQwi6U2NiS"
      },
      "source": [
        "Next we define the dualing network architecture  ( part 2.2.1 )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiNYIbNM30_-"
      },
      "source": [
        "class DQNetwork(nn.Module) : \n",
        "     def __init__(self , n_actions , n_hidden = 512 , n_hidden_inter = 256 , history_length = 4 ):\n",
        "         super().__init__()\n",
        "\n",
        "         self.conv1 = nn.Conv2d(in_channels= history_length, out_channels= 32 , \n",
        "                                kernel_size=(8,8) , stride=4 , bias = False)\n",
        "         self.bn1 = nn.BatchNorm2d(32)\n",
        "         self.conv2 = nn.Conv2d(in_channels = 32, out_channels= 64 , \n",
        "                                kernel_size=(4,4) , stride=2 , bias = False)\n",
        "         self.bn2 = nn.BatchNorm2d(64)\n",
        "         self.conv3 = nn.Conv2d(in_channels = 64, out_channels= 64 , \n",
        "                                kernel_size=(3,3) , stride=1 , bias = False)\n",
        "         self.bn3 = nn.BatchNorm2d(64)\n",
        "         self.conv4 = nn.Conv2d(in_channels = 64, out_channels= n_hidden , \n",
        "                                kernel_size=(7,7) , stride=1 , bias = False)\n",
        "         \n",
        "         ## split to predict the state value and the advantage function\n",
        "         self.state_pred_hidd = nn.Linear(n_hidden , n_hidden_inter)\n",
        "         self.advantage_pred_hidd = nn.Linear(n_hidden , n_hidden_inter)\n",
        "         \n",
        "         self.state_pred  = nn.Linear(n_hidden_inter , 1)\n",
        "         self.advantage_pred = nn.Linear(n_hidden_inter , n_actions)\n",
        "\n",
        "         self.relu = nn.ReLU()\n",
        "         self.n_actions = n_actions\n",
        "     def forward(self,x) :\n",
        "         out = self.relu(self.bn1(self.conv1(x)))\n",
        "         out = self.relu(self.bn2(self.conv2(out)))\n",
        "         out = self.relu(self.bn3(self.conv3(out)))\n",
        "         out = self.relu(self.conv4(out))\n",
        "         out = out.view(out.shape[0],-1)\n",
        "         ## split \n",
        "         state_value = self.state_pred(self.relu(self.state_pred_hidd(out)))\n",
        "         advantage =  self.advantage_pred(self.relu(self.advantage_pred_hidd(out)))\n",
        "         ## define Q function\n",
        "         Q_pred = state_value + advantage - torch.mean(advantage , dim = 1 , keepdim = True)\n",
        "\n",
        "         \n",
        "         return Q_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrTjS9wEBlIW"
      },
      "source": [
        "Here we define the function for training ( part 2.2.2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkteeasnBf7y"
      },
      "source": [
        "\n",
        "def train_batch(minibatch , gamma , n_actions ) : \n",
        "   \"\"\"\n",
        "   Perform a gradient descent step given a minibatch of (states , actions , rewards , next states and terminal flags)\n",
        "   \"\"\"\n",
        "   states , actions , rewards , next_states , terminal_flags = minibatch\n",
        "   states , next_states , actions  = states.to(device , dtype=torch.float32) , next_states.to(device, dtype=torch.float32) , actions.to(device)\n",
        "   Q_pred = main_model(states)   \n",
        "   # get only the predictions corresponding to the given actions. \n",
        "   Q_pred = torch.gather(Q_pred,1,actions.unsqueeze(1)).squeeze()\n",
        "   rewards = rewards.to(device)\n",
        "   terminal_flags = terminal_flags.to(device)\n",
        "   \n",
        "   with torch.no_grad(): \n",
        "        next_states_q_values = main_model(next_states)\n",
        "        next_states_target_q_values = target_model(next_states)\n",
        "        best_action_main = torch.argmax(next_states_q_values , dim = 1)\n",
        "        Q_pred_next = next_states_target_q_values.gather(1 ,best_action_main.unsqueeze(1)).squeeze(1)\n",
        "        Q_target = rewards + gamma * Q_pred_next * (1-terminal_flags)\n",
        "   \n",
        "   optimizer.zero_grad()\n",
        "   loss = hubert_loss (Q_pred , Q_target )\n",
        "   loss.backward()\n",
        "   # clamp the gradients to avoid gradient explosion. \n",
        "   for param in main_model.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "   optimizer.step()\n",
        "   \n",
        "   return loss.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WcjJKFiJmCS"
      },
      "source": [
        "Next we will define the replay memory to sample from when learning ( part 2.3 )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJW-hW41LM2G"
      },
      "source": [
        "class MEMORY(object) :\n",
        "     \"\"\"\n",
        "     Implements replay memory class. \n",
        "     \"\"\"\n",
        "     def __init__(self , batch_size = 32 , img_size = (84,84) , max_transitions = 100000 , frames_stack= 4):\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        self.max_transitions = max_transitions\n",
        "        self.frames =  np.empty((max_transitions,img_size[0],img_size[1]), dtype = np.uint8)\n",
        "        self.actions = np.empty(max_transitions , dtype = np.int32)\n",
        "        self.rewards = np.empty(max_transitions , dtype = np.float32)\n",
        "        self.terminal_flags = np.empty(max_transitions, dtype=np.bool)\n",
        "        self.current = 0 \n",
        "        self.count = 0\n",
        "        self.frames_stack = frames_stack\n",
        "\n",
        "     def add_experience(self , action, frame, reward, terminal) : \n",
        "        \"\"\"\n",
        "        Given a new transition , add it to the memory object. If the number exceeds the maximum number of transitions,\n",
        "        we replace the first transitions in a \"cyclic\" way. \n",
        "        \"\"\"\n",
        "        self.frames [self.current] = frame\n",
        "        self.actions[self.current] = action \n",
        "        self.rewards[self.current] = reward\n",
        "        self.terminal_flags[self.current] = terminal\n",
        "        self.count = max(self.current+1 , self.count )\n",
        "        self.current = (self.current+1) % self.max_transitions\n",
        "    \n",
        "     def sample_minibatch(self) :\n",
        "         \"\"\"\n",
        "         Sample randomly a minibatch. Concretely we sample an index i , the frames corresponding to (i,i+1,i+2,i+3) \n",
        "         are concatenated to yield the current state and (i+1,i+2,i+3,i+4) are concatenated to yield the next state. \n",
        "         \"\"\"\n",
        "         states , next_states , actions_batch , rewards_batch, terminal_batch = [], [], [], [], []\n",
        "         for i in range(self.batch_size) :\n",
        "             while True :\n",
        "                 index = np.random.randint(low = self.frames_stack , high = self.count )\n",
        "                 if self.terminal_flags[index - self.frames_stack:index].any() == False:\n",
        "                    break \n",
        "             current_state = torch.from_numpy(self.frames[index - self.frames_stack:index,...]/255).unsqueeze(0)\n",
        "             next_state = torch.from_numpy(self.frames[index - self.frames_stack + 1:index+1,...]/255).unsqueeze(0)\n",
        "             states.append(current_state)\n",
        "             next_states.append(next_state)\n",
        "             actions_batch.append(self.actions[index])\n",
        "             rewards_batch.append(self.rewards[index])\n",
        "             terminal_batch.append(int(self.terminal_flags[index]))\n",
        "         return (torch.cat(states, dim = 0), torch.tensor(actions_batch , dtype= torch.int64), torch.tensor(rewards_batch),\n",
        "                 torch.cat(next_states , dim = 0), torch.tensor(terminal_batch))\n",
        "        \n",
        "                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Du8AGvS4BOnA"
      },
      "source": [
        "Next we implement epsilon greedy strategy ( part 2.4 ) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAEZVwqEBwoR"
      },
      "source": [
        "class EpsilonGreedy():\n",
        "      \"\"\"\n",
        "      Implements epsilon greedy strategy with decay. \n",
        "      \"\"\"\n",
        "      def __init__(self , n_actions , eps_start = 1. , eps_interm = 0.1 , eps_final = 0.01,\n",
        "                   explore_iter = 50000 , decrease_iter = 1000000 , exploit_iter = 25000000) :\n",
        "          \n",
        "          self.explore_iter = explore_iter\n",
        "          self.decrease_iter = decrease_iter \n",
        "          self.exploit_iter = exploit_iter\n",
        "          self.n_actions = n_actions\n",
        "          self.eps_start  = eps_start \n",
        "          self.eps_final = eps_final \n",
        "          self.eps_interm = eps_interm\n",
        "\n",
        "          self.slope = -(self.eps_start - self.eps_interm)/self.decrease_iter\n",
        "          self.intercept = self.eps_start - self.slope*self.explore_iter\n",
        "          self.slope_2 = -(self.eps_interm - self.eps_final)/(self.exploit_iter - self.decrease_iter - self.explore_iter)\n",
        "          self.intercept_2 = self.eps_final - self.slope_2*self.exploit_iter\n",
        "\n",
        "      def get_epsilon(self , frame_iter , eval) :\n",
        "\n",
        "          if eval :\n",
        "             return 0. ## only exploitation at test time\n",
        "          if frame_iter < self.explore_iter :\n",
        "             return self.eps_start\n",
        "          elif frame_iter >= self.explore_iter and frame_iter < self.decrease_iter + self.explore_iter:\n",
        "               return self.slope * frame_iter + self.intercept\n",
        "          elif frame_iter >= self.decrease_iter + self.explore_iter  :\n",
        "               return self.slope_2 * frame_iter + self.intercept_2\n",
        "\n",
        "      def sample_action (self , frame_iter , state , eval= False) :\n",
        "          \"\"\"\n",
        "          Sample an action given an updated value of epsilon. \n",
        "          \"\"\"\n",
        "          epsilon = self.get_epsilon(frame_iter,eval)\n",
        "          a = random.random()\n",
        "          if a < epsilon :\n",
        "             selected_action = np.random.choice(np.arange(self.n_actions) , 1)[0]\n",
        "             return selected_action\n",
        "          else :\n",
        "             with torch.no_grad() :\n",
        "                tensor_state = torch.from_numpy(state/255).unsqueeze(0).to(device , dtype=torch.float32)\n",
        "                Q_pred = main_model(tensor_state.permute((0,3,1,2)))\n",
        "             return torch.argmax(Q_pred , dim = 1)[0].item()\n",
        "          "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jaxWoV-Wp2R"
      },
      "source": [
        "Next we define the agent class. It containts mainly the methods : reset to restart a game and step to perform an action."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsSSdsEOW2Ki"
      },
      "source": [
        "class AGENT(object) :\n",
        "      \"\"\"\n",
        "      Agent class that reimplements gym ai methods of reset and step. \n",
        "      params : \n",
        "          env_name : in our case will be PongDeterministic-v4. \n",
        "          no_op_steps : action 1 fire is repeated for a random number of steps between 1 and no_op_steps.\n",
        "                        This ensures that the agent starts in different situation when we reset the game.\n",
        "          frames_stacked : the number of frames to stack to get a state. \n",
        "      \"\"\"\n",
        "      def __init__(self, env_name, no_op_steps=10, frames_stacked=4) :\n",
        "         self.env = gym.make(env_name)\n",
        "         self.state = None\n",
        "         self.no_op_steps = no_op_steps\n",
        "         self.frames_stacked = frames_stacked\n",
        "\n",
        "      def reset(self, evaluation=False) :\n",
        "          frame = self.env.reset()\n",
        "          if evaluation:\n",
        "              for _ in range(random.randint(1, self.no_op_steps)):\n",
        "                  frame, _, _, _ = self.env.step(1) # Action 'Fire'\n",
        "\n",
        "          processed_frame = ProcessFrame(frame)  \n",
        "          self.state = np.repeat(processed_frame, self.frames_stacked, axis=2)\n",
        "          \n",
        "      def step(self, action):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            action: Integer, action the agent performs\n",
        "        Performs an action and observes the reward and terminal state from the environment\n",
        "        \"\"\"\n",
        "\n",
        "        new_frame, reward, terminal, info = self.env.step(action) \n",
        "        processed_new_frame = ProcessFrame(new_frame) \n",
        "        new_state = np.append(self.state[:, :, 1:], processed_new_frame, axis=2)  \n",
        "        self.state = new_state\n",
        "        return processed_new_frame, reward, terminal , new_frame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Al45s3sXmklt"
      },
      "source": [
        "import imageio\n",
        "from skimage.transform import resize\n",
        "\n",
        "def clip_reward(reward):\n",
        "    if reward > 0:\n",
        "        return 1\n",
        "    elif reward == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "def generate_gif(n_frame, frames_for_gif, reward, path):\n",
        "    \"\"\"\n",
        "        Args:\n",
        "            n_frame: Integer, determining the number of the current frame\n",
        "            frames_for_gif: A sequence of (210, 160, 3) frames of an Atari game in RGB\n",
        "            reward: Integer, Total reward of the episode that es ouputted as a gif\n",
        "            path: String, path where gif is saved\n",
        "    \"\"\"\n",
        "    for idx, frame_idx in enumerate(frames_for_gif): \n",
        "        frames_for_gif[idx] = resize(frame_idx, (420, 320, 3), \n",
        "                                     preserve_range=True, order=0).astype(np.uint8)\n",
        "        \n",
        "    imageio.mimsave(f'{path}{\"ATARI_frame_{0}_reward_{1}.gif\".format(n_frame, reward)}', \n",
        "                    frames_for_gif, duration=1/30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wWi-UtOUW2m"
      },
      "source": [
        "# Results ( part 2.5 in report )  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPpeGQmRVBRU"
      },
      "source": [
        "First we define main and target models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZtjrRccVHQ0"
      },
      "source": [
        "agent = AGENT(env_name=env_name)\n",
        "\n",
        "## models\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "main_model = DQNetwork(n_actions = agent.env.action_space.n)\n",
        "main_model.to(device)\n",
        "target_model = DQNetwork(n_actions= agent.env.action_space.n)\n",
        "target_model.to(device)\n",
        "target_model.load_state_dict(main_model.state_dict())\n",
        "target_model.eval()\n",
        "\n",
        "## memory \n",
        "memory = MEMORY(max_transitions=1000000)\n",
        "epsilon_strategy = EpsilonGreedy(n_actions= agent.env.action_space.n , exploit_iter = 30000000 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt6HQrzXVHyz"
      },
      "source": [
        "Next we define the set of configurations and hyperparameters for the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhI9wFTuzoCg"
      },
      "source": [
        "learning_rate = 0.0001\n",
        "\n",
        "hubert_loss = nn.SmoothL1Loss()\n",
        "optimizer = torch.optim.Adam(main_model.parameters(), lr=learning_rate)\n",
        "\n",
        "gamma = 0.99\n",
        "eval_freq  =  200 000  ## evaluate every eval_freq steps\n",
        "eval_steps = 10000    ## Number of frames for one evaluation\n",
        "update_target = 10000 ## update target network weights with the main model once every update_target steps\n",
        "max_episode_length = 18000    \n",
        "max_frames = 30000000\n",
        "start_memory = 50000 \n",
        "update_main = 4 # Every update_main actions a gradient descend step is performed\n",
        "\n",
        "## Paths \n",
        "path_weights = '/content/drive/MyDrive/data_CS_MVA/RL/second experience/model_state.pth'\n",
        "path_save = '/content/drive/MyDrive/data_CS_MVA/RL/second experience/'\n",
        "path_results = '/content/drive/MyDrive/data_CS_MVA/RL/second experience/dict_res.pkl'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1V9OdlgXWl4N"
      },
      "source": [
        "Next is the main cell , where we train and evaluate. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0173eaBdCHB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1445a4ce-595e-4a77-f27f-767806e256fd"
      },
      "source": [
        "n_frame = 0\n",
        "rewards = []\n",
        "loss_list = []\n",
        "dict_results = {'loss': [], 'rewards': [] , 'rewards-eval': []}\n",
        "while n_frame < max_frames :\n",
        "      ############# train ################### \n",
        "      epoch = 0 \n",
        "      while epoch < eval_freq :\n",
        "            agent.reset()\n",
        "            episode_reward_sum = 0\n",
        "            for _ in range(max_episode_length) :\n",
        "                selected_action = epsilon_strategy.sample_action(n_frame , agent.state )\n",
        "                processed_new_frame , reward, terminal, _ = agent.step(selected_action)\n",
        "                n_frame += 1\n",
        "                epoch += 1\n",
        "                episode_reward_sum += reward\n",
        "                \n",
        "                # Clip the reward\n",
        "                clipped_reward = clip_reward(reward)\n",
        "                \n",
        "                #  Store transition in the replay memory\n",
        "                memory.add_experience(action=selected_action, \n",
        "                                      frame=processed_new_frame[:, :, 0],\n",
        "                                      reward=clipped_reward, \n",
        "                                      terminal=terminal)   \n",
        "                #update main network weights every upadte_main frames, and when memory contains enough frames (start_memory)\n",
        "                if  n_frame % update_main == 0 and n_frame > start_memory:\n",
        "                    minibatch = memory.sample_minibatch()\n",
        "                    loss = train_batch(minibatch = minibatch , gamma = gamma , \n",
        "                                       n_actions = agent.env.action_space.n)\n",
        "                    loss_list.append(loss)\n",
        "\n",
        "                #update target network weights every upadte_target frames, and when memory contains enough frames (start_memory)\n",
        "                if n_frame % update_target == 0 and n_frame > start_memory:\n",
        "                    target_model.load_state_dict(main_model.state_dict())\n",
        "                \n",
        "                #if finished break the loop\n",
        "                if terminal:\n",
        "                    terminal = False\n",
        "                    break\n",
        "            \n",
        "            rewards.append(episode_reward_sum)\n",
        "            # Output the progress:\n",
        "            if len(rewards) % 10 == 0:\n",
        "                if n_frame >  start_memory:\n",
        "                    dict_results['loss'].append(loss_list)\n",
        "                    dict_results['rewards'].append([len(rewards), n_frame, \n",
        "                          np.mean(rewards[-100:])])\n",
        "                    pickle.dump(dict_results, open(path_results , 'wb') )\n",
        "                    loss_list = []\n",
        "\n",
        "                \n",
        "                print('We are at episode {} , frame number is {} : mean rewards for the last 100 episodes is {}'.format(len(rewards), n_frame, \n",
        "                          np.mean(rewards[-100:])))\n",
        "              \n",
        "      ####################### evaluate ############################\n",
        "      terminal = True\n",
        "      gif = True\n",
        "      frames_for_gif = []\n",
        "      eval_rewards = []\n",
        "      \n",
        "      for _ in range(eval_steps):\n",
        "          if terminal:\n",
        "              agent.reset(evaluation=True)\n",
        "              episode_reward_sum = 0\n",
        "              terminal = False\n",
        "          \n",
        "          # Fire (action 1), when a life was lost or the game just started, \n",
        "          # so that the agent does not stand around doing nothing. When playing \n",
        "          # with other environments, you might want to change this...\n",
        "          selected_action = epsilon_strategy.sample_action(n_frame , agent.state , eval = True)\n",
        "          \n",
        "          processed_new_frame, reward, terminal, new_frame = agent.step(selected_action)\n",
        "          episode_reward_sum += reward\n",
        "\n",
        "          if gif: \n",
        "              frames_for_gif.append(new_frame)\n",
        "          if terminal:\n",
        "              eval_rewards.append(episode_reward_sum)\n",
        "              gif = False # Save only the first game of the evaluation as a gif\n",
        "                \n",
        "      print(\"Evaluation score:\\n\", np.mean(eval_rewards))       \n",
        "      try:\n",
        "          generate_gif(n_frame, frames_for_gif, eval_rewards[0], path_save)\n",
        "      except IndexError:\n",
        "          print(\"No evaluation game finished\")\n",
        "      \n",
        "      #Save the network parameters\n",
        "      torch.save(main_model.state_dict(), path_weights)\n",
        "      frames_for_gif = []\n",
        "      \n",
        "      # Show the evaluation score in tensorboard\n",
        "      dict_results['rewards-eval'].append([n_frame, np.mean(eval_rewards)])\n",
        "      pickle.dump(dict_results, open(path_results , 'wb') )        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We are at episode 10 , frame number is 8770 : mean rewards for the last 100 episodes is -20.5\n",
            "We are at episode 20 , frame number is 18120 : mean rewards for the last 100 episodes is -20.4\n",
            "We are at episode 30 , frame number is 27331 : mean rewards for the last 100 episodes is -20.333333333333332\n",
            "We are at episode 40 , frame number is 36590 : mean rewards for the last 100 episodes is -20.35\n",
            "We are at episode 50 , frame number is 45851 : mean rewards for the last 100 episodes is -20.32\n",
            "We are at episode 60 , frame number is 55405 : mean rewards for the last 100 episodes is -20.266666666666666\n",
            "We are at episode 70 , frame number is 65022 : mean rewards for the last 100 episodes is -20.257142857142856\n",
            "We are at episode 80 , frame number is 74484 : mean rewards for the last 100 episodes is -20.2875\n",
            "We are at episode 90 , frame number is 83324 : mean rewards for the last 100 episodes is -20.344444444444445\n",
            "We are at episode 100 , frame number is 92378 : mean rewards for the last 100 episodes is -20.34\n",
            "We are at episode 110 , frame number is 101374 : mean rewards for the last 100 episodes is -20.34\n",
            "We are at episode 120 , frame number is 110699 : mean rewards for the last 100 episodes is -20.33\n",
            "We are at episode 130 , frame number is 120033 : mean rewards for the last 100 episodes is -20.32\n",
            "We are at episode 140 , frame number is 129133 : mean rewards for the last 100 episodes is -20.34\n",
            "We are at episode 150 , frame number is 138190 : mean rewards for the last 100 episodes is -20.34\n",
            "We are at episode 160 , frame number is 147900 : mean rewards for the last 100 episodes is -20.34\n",
            "We are at episode 170 , frame number is 156760 : mean rewards for the last 100 episodes is -20.38\n",
            "We are at episode 180 , frame number is 165792 : mean rewards for the last 100 episodes is -20.42\n",
            "We are at episode 190 , frame number is 175429 : mean rewards for the last 100 episodes is -20.35\n",
            "We are at episode 200 , frame number is 183680 : mean rewards for the last 100 episodes is -20.4\n",
            "We are at episode 210 , frame number is 192987 : mean rewards for the last 100 episodes is -20.41\n",
            "Evaluation score:\n",
            " -21.0\n",
            "We are at episode 220 , frame number is 202497 : mean rewards for the last 100 episodes is -20.42\n",
            "We are at episode 230 , frame number is 211970 : mean rewards for the last 100 episodes is -20.45\n",
            "We are at episode 240 , frame number is 221555 : mean rewards for the last 100 episodes is -20.39\n",
            "We are at episode 250 , frame number is 230941 : mean rewards for the last 100 episodes is -20.39\n",
            "We are at episode 260 , frame number is 241712 : mean rewards for the last 100 episodes is -20.31\n",
            "We are at episode 270 , frame number is 252098 : mean rewards for the last 100 episodes is -20.23\n",
            "We are at episode 280 , frame number is 261789 : mean rewards for the last 100 episodes is -20.15\n",
            "We are at episode 290 , frame number is 271479 : mean rewards for the last 100 episodes is -20.18\n",
            "We are at episode 300 , frame number is 281686 : mean rewards for the last 100 episodes is -20.1\n",
            "We are at episode 310 , frame number is 291673 : mean rewards for the last 100 episodes is -20.02\n",
            "We are at episode 320 , frame number is 300663 : mean rewards for the last 100 episodes is -20.08\n",
            "We are at episode 330 , frame number is 310420 : mean rewards for the last 100 episodes is -20.07\n",
            "We are at episode 340 , frame number is 319067 : mean rewards for the last 100 episodes is -20.15\n",
            "We are at episode 350 , frame number is 328684 : mean rewards for the last 100 episodes is -20.18\n",
            "We are at episode 360 , frame number is 338558 : mean rewards for the last 100 episodes is -20.27\n",
            "We are at episode 370 , frame number is 347713 : mean rewards for the last 100 episodes is -20.34\n",
            "We are at episode 380 , frame number is 357082 : mean rewards for the last 100 episodes is -20.34\n",
            "We are at episode 390 , frame number is 366421 : mean rewards for the last 100 episodes is -20.33\n",
            "We are at episode 400 , frame number is 375453 : mean rewards for the last 100 episodes is -20.36\n",
            "We are at episode 410 , frame number is 384528 : mean rewards for the last 100 episodes is -20.44\n",
            "We are at episode 420 , frame number is 393251 : mean rewards for the last 100 episodes is -20.42\n",
            "Evaluation score:\n",
            " -21.0\n",
            "We are at episode 430 , frame number is 403134 : mean rewards for the last 100 episodes is -20.4\n",
            "We are at episode 440 , frame number is 413423 : mean rewards for the last 100 episodes is -20.36\n",
            "We are at episode 450 , frame number is 422698 : mean rewards for the last 100 episodes is -20.34\n",
            "We are at episode 460 , frame number is 431826 : mean rewards for the last 100 episodes is -20.37\n",
            "We are at episode 470 , frame number is 440776 : mean rewards for the last 100 episodes is -20.39\n",
            "We are at episode 480 , frame number is 449996 : mean rewards for the last 100 episodes is -20.43\n",
            "We are at episode 490 , frame number is 458834 : mean rewards for the last 100 episodes is -20.48\n",
            "We are at episode 500 , frame number is 468606 : mean rewards for the last 100 episodes is -20.47\n",
            "We are at episode 510 , frame number is 477365 : mean rewards for the last 100 episodes is -20.46\n",
            "We are at episode 520 , frame number is 486645 : mean rewards for the last 100 episodes is -20.44\n",
            "We are at episode 530 , frame number is 495388 : mean rewards for the last 100 episodes is -20.48\n",
            "We are at episode 540 , frame number is 504269 : mean rewards for the last 100 episodes is -20.51\n",
            "We are at episode 550 , frame number is 512877 : mean rewards for the last 100 episodes is -20.53\n",
            "We are at episode 560 , frame number is 521977 : mean rewards for the last 100 episodes is -20.51\n",
            "We are at episode 570 , frame number is 530591 : mean rewards for the last 100 episodes is -20.45\n",
            "We are at episode 580 , frame number is 539816 : mean rewards for the last 100 episodes is -20.42\n",
            "We are at episode 590 , frame number is 548737 : mean rewards for the last 100 episodes is -20.41\n",
            "We are at episode 600 , frame number is 557695 : mean rewards for the last 100 episodes is -20.41\n",
            "We are at episode 610 , frame number is 567195 : mean rewards for the last 100 episodes is -20.37\n",
            "We are at episode 620 , frame number is 576567 : mean rewards for the last 100 episodes is -20.39\n",
            "We are at episode 630 , frame number is 585598 : mean rewards for the last 100 episodes is -20.37\n",
            "We are at episode 640 , frame number is 594679 : mean rewards for the last 100 episodes is -20.34\n",
            "Evaluation score:\n",
            " -21.0\n",
            "We are at episode 650 , frame number is 603883 : mean rewards for the last 100 episodes is -20.35\n",
            "We are at episode 660 , frame number is 612559 : mean rewards for the last 100 episodes is -20.38\n",
            "We are at episode 670 , frame number is 621271 : mean rewards for the last 100 episodes is -20.44\n",
            "We are at episode 680 , frame number is 629829 : mean rewards for the last 100 episodes is -20.46\n",
            "We are at episode 690 , frame number is 638465 : mean rewards for the last 100 episodes is -20.46\n",
            "We are at episode 700 , frame number is 647209 : mean rewards for the last 100 episodes is -20.51\n",
            "We are at episode 710 , frame number is 658487 : mean rewards for the last 100 episodes is -20.42\n",
            "We are at episode 720 , frame number is 670433 : mean rewards for the last 100 episodes is -20.26\n",
            "We are at episode 730 , frame number is 684519 : mean rewards for the last 100 episodes is -20.07\n",
            "We are at episode 740 , frame number is 700501 : mean rewards for the last 100 episodes is -19.71\n",
            "We are at episode 750 , frame number is 716649 : mean rewards for the last 100 episodes is -19.33\n",
            "We are at episode 760 , frame number is 733147 : mean rewards for the last 100 episodes is -19.07\n",
            "We are at episode 770 , frame number is 751147 : mean rewards for the last 100 episodes is -18.65\n",
            "We are at episode 780 , frame number is 766239 : mean rewards for the last 100 episodes is -18.35\n",
            "We are at episode 790 , frame number is 780746 : mean rewards for the last 100 episodes is -18.07\n",
            "We are at episode 800 , frame number is 797294 : mean rewards for the last 100 episodes is -17.76\n",
            "Evaluation score:\n",
            " -21.0\n",
            "We are at episode 810 , frame number is 815132 : mean rewards for the last 100 episodes is -17.55\n",
            "We are at episode 820 , frame number is 835958 : mean rewards for the last 100 episodes is -17.16\n",
            "We are at episode 830 , frame number is 855461 : mean rewards for the last 100 episodes is -17.04\n",
            "We are at episode 840 , frame number is 876283 : mean rewards for the last 100 episodes is -16.97\n",
            "We are at episode 850 , frame number is 906670 : mean rewards for the last 100 episodes is -16.57\n",
            "We are at episode 860 , frame number is 933600 : mean rewards for the last 100 episodes is -16.2\n",
            "We are at episode 870 , frame number is 959193 : mean rewards for the last 100 episodes is -16.02\n",
            "We are at episode 880 , frame number is 983997 : mean rewards for the last 100 episodes is -15.82\n",
            "Evaluation score:\n",
            " -20.0\n",
            "We are at episode 890 , frame number is 1014487 : mean rewards for the last 100 episodes is -15.24\n",
            "We are at episode 900 , frame number is 1042795 : mean rewards for the last 100 episodes is -14.69\n",
            "We are at episode 910 , frame number is 1071568 : mean rewards for the last 100 episodes is -14.24\n",
            "We are at episode 920 , frame number is 1102578 : mean rewards for the last 100 episodes is -13.89\n",
            "We are at episode 930 , frame number is 1132742 : mean rewards for the last 100 episodes is -13.4\n",
            "We are at episode 940 , frame number is 1167468 : mean rewards for the last 100 episodes is -12.96\n",
            "We are at episode 950 , frame number is 1203520 : mean rewards for the last 100 episodes is -12.65\n",
            "Evaluation score:\n",
            " -20.0\n",
            "We are at episode 960 , frame number is 1236122 : mean rewards for the last 100 episodes is -12.14\n",
            "We are at episode 970 , frame number is 1266148 : mean rewards for the last 100 episodes is -12.05\n",
            "We are at episode 980 , frame number is 1297667 : mean rewards for the last 100 episodes is -11.81\n",
            "We are at episode 990 , frame number is 1330536 : mean rewards for the last 100 episodes is -11.47\n",
            "We are at episode 1000 , frame number is 1358855 : mean rewards for the last 100 episodes is -11.64\n",
            "We are at episode 1010 , frame number is 1390390 : mean rewards for the last 100 episodes is -11.65\n",
            "Evaluation score:\n",
            " -16.0\n",
            "We are at episode 1020 , frame number is 1423845 : mean rewards for the last 100 episodes is -11.44\n",
            "We are at episode 1030 , frame number is 1457383 : mean rewards for the last 100 episodes is -11.47\n",
            "We are at episode 1040 , frame number is 1497001 : mean rewards for the last 100 episodes is -11.18\n",
            "We are at episode 1050 , frame number is 1532132 : mean rewards for the last 100 episodes is -11.34\n",
            "We are at episode 1060 , frame number is 1564715 : mean rewards for the last 100 episodes is -11.66\n",
            "We are at episode 1070 , frame number is 1582142 : mean rewards for the last 100 episodes is -12.08\n",
            "We are at episode 1080 , frame number is 1606245 : mean rewards for the last 100 episodes is -12.46\n",
            "Evaluation score:\n",
            " 20.0\n",
            "We are at episode 1090 , frame number is 1641056 : mean rewards for the last 100 episodes is -12.92\n",
            "We are at episode 1100 , frame number is 1678029 : mean rewards for the last 100 episodes is -12.68\n",
            "We are at episode 1110 , frame number is 1714778 : mean rewards for the last 100 episodes is -12.7\n",
            "We are at episode 1120 , frame number is 1749223 : mean rewards for the last 100 episodes is -13.1\n",
            "We are at episode 1130 , frame number is 1791873 : mean rewards for the last 100 episodes is -12.58\n",
            "Evaluation score:\n",
            " 1.0\n",
            "We are at episode 1140 , frame number is 1832961 : mean rewards for the last 100 episodes is -11.55\n",
            "We are at episode 1150 , frame number is 1867976 : mean rewards for the last 100 episodes is -10.34\n",
            "We are at episode 1160 , frame number is 1911343 : mean rewards for the last 100 episodes is -9.24\n",
            "We are at episode 1170 , frame number is 1955335 : mean rewards for the last 100 episodes is -8.12\n",
            "We are at episode 1180 , frame number is 1996985 : mean rewards for the last 100 episodes is -6.39\n",
            "Evaluation score:\n",
            " 8.0\n",
            "We are at episode 1190 , frame number is 2043093 : mean rewards for the last 100 episodes is -5.27\n",
            "We are at episode 1200 , frame number is 2086641 : mean rewards for the last 100 episodes is -4.54\n",
            "We are at episode 1210 , frame number is 2124820 : mean rewards for the last 100 episodes is -3.05\n",
            "We are at episode 1220 , frame number is 2161986 : mean rewards for the last 100 episodes is -1.68\n",
            "We are at episode 1230 , frame number is 2194742 : mean rewards for the last 100 episodes is -0.55\n",
            "Evaluation score:\n",
            " -19.0\n",
            "We are at episode 1240 , frame number is 2218111 : mean rewards for the last 100 episodes is -1.87\n",
            "We are at episode 1250 , frame number is 2250597 : mean rewards for the last 100 episodes is -3.07\n",
            "We are at episode 1260 , frame number is 2285434 : mean rewards for the last 100 episodes is -4.1\n",
            "We are at episode 1270 , frame number is 2326322 : mean rewards for the last 100 episodes is -3.87\n",
            "We are at episode 1280 , frame number is 2365514 : mean rewards for the last 100 episodes is -4.8\n",
            "We are at episode 1290 , frame number is 2388027 : mean rewards for the last 100 episodes is -5.83\n",
            "Evaluation score:\n",
            " -20.0\n",
            "We are at episode 1300 , frame number is 2421953 : mean rewards for the last 100 episodes is -5.65\n",
            "We are at episode 1310 , frame number is 2448599 : mean rewards for the last 100 episodes is -6.8\n",
            "We are at episode 1320 , frame number is 2482814 : mean rewards for the last 100 episodes is -7.03\n",
            "We are at episode 1330 , frame number is 2520173 : mean rewards for the last 100 episodes is -7.08\n",
            "We are at episode 1340 , frame number is 2551356 : mean rewards for the last 100 episodes is -5.8\n",
            "We are at episode 1350 , frame number is 2583727 : mean rewards for the last 100 episodes is -4.38\n",
            "We are at episode 1360 , frame number is 2609838 : mean rewards for the last 100 episodes is -2.11\n",
            "Evaluation score:\n",
            " 21.0\n",
            "We are at episode 1370 , frame number is 2632712 : mean rewards for the last 100 episodes is -0.2\n",
            "We are at episode 1380 , frame number is 2658115 : mean rewards for the last 100 episodes is 1.99\n",
            "We are at episode 1390 , frame number is 2683900 : mean rewards for the last 100 episodes is 4.44\n",
            "We are at episode 1400 , frame number is 2709563 : mean rewards for the last 100 episodes is 5.02\n",
            "We are at episode 1410 , frame number is 2739297 : mean rewards for the last 100 episodes is 5.38\n",
            "We are at episode 1420 , frame number is 2770358 : mean rewards for the last 100 episodes is 6.53\n",
            "We are at episode 1430 , frame number is 2801530 : mean rewards for the last 100 episodes is 7.03\n",
            "Evaluation score:\n",
            " 19.0\n",
            "We are at episode 1440 , frame number is 2833396 : mean rewards for the last 100 episodes is 7.37\n",
            "We are at episode 1450 , frame number is 2866747 : mean rewards for the last 100 episodes is 7.39\n",
            "We are at episode 1460 , frame number is 2894120 : mean rewards for the last 100 episodes is 7.39\n",
            "We are at episode 1470 , frame number is 2924987 : mean rewards for the last 100 episodes is 5.87\n",
            "We are at episode 1480 , frame number is 2957261 : mean rewards for the last 100 episodes is 5.17\n",
            "We are at episode 1490 , frame number is 2990375 : mean rewards for the last 100 episodes is 3.27\n",
            "We are at episode 1500 , frame number is 3024714 : mean rewards for the last 100 episodes is 2.89\n",
            "Evaluation score:\n",
            " 13.0\n",
            "We are at episode 1510 , frame number is 3059119 : mean rewards for the last 100 episodes is 3.48\n",
            "We are at episode 1520 , frame number is 3092808 : mean rewards for the last 100 episodes is 2.9\n",
            "We are at episode 1530 , frame number is 3119786 : mean rewards for the last 100 episodes is 3.09\n",
            "We are at episode 1540 , frame number is 3146559 : mean rewards for the last 100 episodes is 3.94\n",
            "We are at episode 1550 , frame number is 3177633 : mean rewards for the last 100 episodes is 4.47\n",
            "We are at episode 1560 , frame number is 3203106 : mean rewards for the last 100 episodes is 4.7\n",
            "We are at episode 1570 , frame number is 3226279 : mean rewards for the last 100 episodes is 6.22\n",
            "Evaluation score:\n",
            " 21.0\n",
            "We are at episode 1580 , frame number is 3251084 : mean rewards for the last 100 episodes is 6.87\n",
            "We are at episode 1590 , frame number is 3275978 : mean rewards for the last 100 episodes is 8.98\n",
            "We are at episode 1600 , frame number is 3301157 : mean rewards for the last 100 episodes is 10.36\n",
            "We are at episode 1610 , frame number is 3327844 : mean rewards for the last 100 episodes is 11.43\n",
            "We are at episode 1620 , frame number is 3351214 : mean rewards for the last 100 episodes is 12.55\n",
            "We are at episode 1630 , frame number is 3378701 : mean rewards for the last 100 episodes is 12.56\n",
            "We are at episode 1640 , frame number is 3403472 : mean rewards for the last 100 episodes is 12.61\n",
            "Evaluation score:\n",
            " 21.0\n",
            "We are at episode 1650 , frame number is 3430895 : mean rewards for the last 100 episodes is 12.82\n",
            "We are at episode 1660 , frame number is 3454645 : mean rewards for the last 100 episodes is 12.92\n",
            "We are at episode 1670 , frame number is 3480462 : mean rewards for the last 100 episodes is 12.74\n",
            "We are at episode 1680 , frame number is 3504054 : mean rewards for the last 100 episodes is 12.81\n",
            "We are at episode 1690 , frame number is 3526178 : mean rewards for the last 100 episodes is 12.98\n",
            "We are at episode 1700 , frame number is 3554240 : mean rewards for the last 100 episodes is 12.46\n",
            "We are at episode 1710 , frame number is 3580542 : mean rewards for the last 100 episodes is 12.62\n",
            "We are at episode 1720 , frame number is 3609290 : mean rewards for the last 100 episodes is 11.39\n",
            "Evaluation score:\n",
            " 15.0\n",
            "We are at episode 1730 , frame number is 3636898 : mean rewards for the last 100 episodes is 9.58\n",
            "We are at episode 1740 , frame number is 3671456 : mean rewards for the last 100 episodes is 7.82\n",
            "We are at episode 1750 , frame number is 3705584 : mean rewards for the last 100 episodes is 7.33\n",
            "We are at episode 1760 , frame number is 3737872 : mean rewards for the last 100 episodes is 6.63\n",
            "We are at episode 1770 , frame number is 3763041 : mean rewards for the last 100 episodes is 6.45\n",
            "We are at episode 1780 , frame number is 3787698 : mean rewards for the last 100 episodes is 6.39\n",
            "We are at episode 1790 , frame number is 3811220 : mean rewards for the last 100 episodes is 6.31\n",
            "Evaluation score:\n",
            " 20.0\n",
            "We are at episode 1800 , frame number is 3837281 : mean rewards for the last 100 episodes is 6.77\n",
            "We are at episode 1810 , frame number is 3859425 : mean rewards for the last 100 episodes is 7.16\n",
            "We are at episode 1820 , frame number is 3882824 : mean rewards for the last 100 episodes is 8.37\n",
            "We are at episode 1830 , frame number is 3905843 : mean rewards for the last 100 episodes is 10.58\n",
            "We are at episode 1840 , frame number is 3928080 : mean rewards for the last 100 episodes is 12.54\n",
            "We are at episode 1850 , frame number is 3950195 : mean rewards for the last 100 episodes is 13.63\n",
            "We are at episode 1860 , frame number is 3972864 : mean rewards for the last 100 episodes is 14.4\n",
            "We are at episode 1870 , frame number is 3994063 : mean rewards for the last 100 episodes is 14.92\n",
            "We are at episode 1880 , frame number is 4017843 : mean rewards for the last 100 episodes is 14.82\n",
            "Evaluation score:\n",
            " 2.0\n",
            "We are at episode 1890 , frame number is 4041975 : mean rewards for the last 100 episodes is 14.75\n",
            "We are at episode 1900 , frame number is 4063970 : mean rewards for the last 100 episodes is 14.9\n",
            "We are at episode 1910 , frame number is 4095051 : mean rewards for the last 100 episodes is 13.87\n",
            "We are at episode 1920 , frame number is 4120842 : mean rewards for the last 100 episodes is 13.64\n",
            "We are at episode 1930 , frame number is 4142808 : mean rewards for the last 100 episodes is 13.78\n",
            "We are at episode 1940 , frame number is 4162029 : mean rewards for the last 100 episodes is 14.08\n",
            "We are at episode 1950 , frame number is 4182594 : mean rewards for the last 100 episodes is 14.16\n",
            "We are at episode 1960 , frame number is 4204894 : mean rewards for the last 100 episodes is 14.12\n",
            "We are at episode 1970 , frame number is 4226454 : mean rewards for the last 100 episodes is 14.05\n",
            "Evaluation score:\n",
            " 21.0\n",
            "We are at episode 1980 , frame number is 4248655 : mean rewards for the last 100 episodes is 14.45\n",
            "We are at episode 1990 , frame number is 4271149 : mean rewards for the last 100 episodes is 14.36\n",
            "We are at episode 2000 , frame number is 4292578 : mean rewards for the last 100 episodes is 11.57\n",
            "We are at episode 2010 , frame number is 4310826 : mean rewards for the last 100 episodes is 9.35\n",
            "We are at episode 2020 , frame number is 4330231 : mean rewards for the last 100 episodes is 6.41\n",
            "We are at episode 2030 , frame number is 4355862 : mean rewards for the last 100 episodes is 3.88\n",
            "We are at episode 2040 , frame number is 4387644 : mean rewards for the last 100 episodes is 1.62\n",
            "We are at episode 2050 , frame number is 4408237 : mean rewards for the last 100 episodes is -1.53\n",
            "We are at episode 2060 , frame number is 4430164 : mean rewards for the last 100 episodes is -4.45\n",
            "Evaluation score:\n",
            " -20.0\n",
            "We are at episode 2070 , frame number is 4450256 : mean rewards for the last 100 episodes is -7.43\n",
            "We are at episode 2080 , frame number is 4477399 : mean rewards for the last 100 episodes is -9.64\n",
            "We are at episode 2090 , frame number is 4506985 : mean rewards for the last 100 episodes is -11.06\n",
            "We are at episode 2100 , frame number is 4537479 : mean rewards for the last 100 episodes is -9.5\n",
            "We are at episode 2110 , frame number is 4567847 : mean rewards for the last 100 episodes is -7.72\n",
            "We are at episode 2120 , frame number is 4594186 : mean rewards for the last 100 episodes is -5.38\n",
            "We are at episode 2130 , frame number is 4620556 : mean rewards for the last 100 episodes is -3.49\n",
            "Evaluation score:\n",
            " -13.0\n",
            "We are at episode 2140 , frame number is 4649951 : mean rewards for the last 100 episodes is -2.9\n",
            "We are at episode 2150 , frame number is 4677899 : mean rewards for the last 100 episodes is -0.9\n",
            "We are at episode 2160 , frame number is 4703423 : mean rewards for the last 100 episodes is 1.67\n",
            "We are at episode 2170 , frame number is 4728103 : mean rewards for the last 100 episodes is 4.43\n",
            "We are at episode 2180 , frame number is 4750400 : mean rewards for the last 100 episodes is 6.52\n",
            "We are at episode 2190 , frame number is 4773707 : mean rewards for the last 100 episodes is 8.0\n",
            "We are at episode 2200 , frame number is 4801858 : mean rewards for the last 100 episodes is 8.75\n",
            "We are at episode 2210 , frame number is 4821540 : mean rewards for the last 100 episodes is 10.32\n",
            "Evaluation score:\n",
            " 21.0\n",
            "We are at episode 2220 , frame number is 4845449 : mean rewards for the last 100 episodes is 11.1\n",
            "We are at episode 2230 , frame number is 4868937 : mean rewards for the last 100 episodes is 11.65\n",
            "We are at episode 2240 , frame number is 4891331 : mean rewards for the last 100 episodes is 12.96\n",
            "We are at episode 2250 , frame number is 4913979 : mean rewards for the last 100 episodes is 13.86\n",
            "We are at episode 2260 , frame number is 4935537 : mean rewards for the last 100 episodes is 14.33\n",
            "We are at episode 2270 , frame number is 4957371 : mean rewards for the last 100 episodes is 14.54\n",
            "We are at episode 2280 , frame number is 4979135 : mean rewards for the last 100 episodes is 14.58\n",
            "We are at episode 2290 , frame number is 5000047 : mean rewards for the last 100 episodes is 14.86\n",
            "We are at episode 2300 , frame number is 5024590 : mean rewards for the last 100 episodes is 15.08\n",
            "Evaluation score:\n",
            " 21.0\n",
            "We are at episode 2310 , frame number is 5049632 : mean rewards for the last 100 episodes is 14.71\n",
            "We are at episode 2320 , frame number is 5071185 : mean rewards for the last 100 episodes is 14.88\n",
            "We are at episode 2330 , frame number is 5092287 : mean rewards for the last 100 episodes is 15.07\n",
            "We are at episode 2340 , frame number is 5107855 : mean rewards for the last 100 episodes is 13.07\n",
            "We are at episode 2350 , frame number is 5127785 : mean rewards for the last 100 episodes is 10.06\n",
            "We are at episode 2360 , frame number is 5148227 : mean rewards for the last 100 episodes is 6.91\n",
            "We are at episode 2370 , frame number is 5181120 : mean rewards for the last 100 episodes is 4.67\n",
            "We are at episode 2380 , frame number is 5217747 : mean rewards for the last 100 episodes is 3.05\n",
            "Evaluation score:\n",
            " 11.0\n",
            "We are at episode 2390 , frame number is 5251917 : mean rewards for the last 100 episodes is 1.42\n",
            "We are at episode 2400 , frame number is 5285957 : mean rewards for the last 100 episodes is 0.45\n",
            "We are at episode 2410 , frame number is 5320227 : mean rewards for the last 100 episodes is -0.73\n",
            "We are at episode 2420 , frame number is 5346811 : mean rewards for the last 100 episodes is -1.25\n",
            "We are at episode 2430 , frame number is 5374029 : mean rewards for the last 100 episodes is -3.84\n",
            "We are at episode 2440 , frame number is 5392641 : mean rewards for the last 100 episodes is -4.98\n",
            "We are at episode 2450 , frame number is 5422475 : mean rewards for the last 100 episodes is -4.58\n",
            "Evaluation score:\n",
            " 9.0\n",
            "We are at episode 2460 , frame number is 5454778 : mean rewards for the last 100 episodes is -3.88\n",
            "We are at episode 2470 , frame number is 5483381 : mean rewards for the last 100 episodes is -3.96\n",
            "We are at episode 2480 , frame number is 5518224 : mean rewards for the last 100 episodes is -3.81\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-0fe6d9a5c600>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mepisode_reward_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_episode_length\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0mselected_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_frame\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                 \u001b[0mprocessed_new_frame\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mn_frame\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-3f343bada451>\u001b[0m in \u001b[0;36msample_action\u001b[0;34m(self, frame_iter, state, eval)\u001b[0m\n\u001b[1;32m     39\u001b[0m              \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mtensor_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0mQ_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m              \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_pred\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-7fbe561e32d8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m          \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m          \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m          \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m          \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m          \u001b[0;31m## split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 869\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backward_hooks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m             \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_backward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dmImxcQkET0",
        "outputId": "126f6fc6-4ae6-496c-de60-96a0000c7b6c"
      },
      "source": [
        "terminal = True\n",
        "gif = True\n",
        "frames_for_gif = []\n",
        "eval_rewards = []\n",
        "\n",
        "for _ in range(eval_steps):\n",
        "    if terminal:\n",
        "        agent.reset(evaluation=True)\n",
        "        episode_reward_sum = 0\n",
        "        terminal = False\n",
        "    \n",
        "    # Fire (action 1), when a life was lost or the game just started, \n",
        "    # so that the agent does not stand around doing nothing. When playing \n",
        "    # with other environments, you might want to change this...\n",
        "    selected_action = epsilon_strategy.sample_action(n_frame , agent.state , eval = True)\n",
        "    \n",
        "    processed_new_frame, reward, terminal, new_frame = agent.step(selected_action)\n",
        "    episode_reward_sum += reward\n",
        "\n",
        "    if gif: \n",
        "        frames_for_gif.append(new_frame)\n",
        "    if terminal:\n",
        "        eval_rewards.append(episode_reward_sum)\n",
        "        gif = False # Save only the first game of the evaluation as a gif\n",
        "          \n",
        "print(\"Evaluation score:\\n\", np.mean(eval_rewards))       \n",
        "try:\n",
        "    generate_gif(n_frame, frames_for_gif, eval_rewards[0], path_save)\n",
        "except IndexError:\n",
        "    print(\"No evaluation game finished\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation score:\n",
            " 21.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}